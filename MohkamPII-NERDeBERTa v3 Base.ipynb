{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPHH5Phgua3LwDHtGWvBTe8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLuLUJnHawu8","executionInfo":{"status":"ok","timestamp":1756181118120,"user_tz":-180,"elapsed":18704,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"b25b9fa4-c119-486d-92c5-63058fb9f3cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os, json"],"metadata":{"id":"tHWe2FhEdaLF","executionInfo":{"status":"ok","timestamp":1756181118140,"user_tz":-180,"elapsed":16,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_j_path = \"/content/drive/MyDrive/Colab Notebooks/bootcamb/train.json\"\n","test_j_path =  \"/content/drive/MyDrive/Colab Notebooks/bootcamb/test.json\""],"metadata":{"id":"7IXPqxVdcWQf","executionInfo":{"status":"ok","timestamp":1756181118176,"user_tz":-180,"elapsed":30,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["assert os.path.exists(train_j_path), f\"train.json not found at: {train_j_path}\"\n","print(\"train.json size (MB):\", round(os.path.getsize(train_j_path)/1e6, 2))\n","\n","# quick peek\n","sample = json.load(open(train_j_path, \"r\"))[:1]\n","print(\"Sample keys:\", list(sample[0].keys()))\n","print(\"Tokens ex:\", sample[0][\"tokens\"][:10])\n","print(\"Labels ex:\", sample[0][\"labels\"][:10])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOxZOfUadBbh","executionInfo":{"status":"ok","timestamp":1756181126162,"user_tz":-180,"elapsed":7991,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"b06b959c-ace3-4f8d-cbd6-5baf95010933"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["train.json size (MB): 109.5\n","Sample keys: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']\n","Tokens ex: ['Design', 'Thinking', 'for', 'innovation', 'reflexion', '-', 'Avril', '2021', '-', 'Nathalie']\n","Labels ex: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT']\n"]}]},{"cell_type":"code","source":["###################################################################\n","###################################################################\n","###################################################################\n","###################################################################\n","###################################################################\n","###################################################################"],"metadata":{"id":"kH5embGS4izf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clean setup: disable TF/Flax to avoid extra imports, keep environment minimal\n","import os\n","os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n","os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","# Minimal installs (no-deps to avoid dragging conflicting packages)\n","!pip -q install --no-deps \"transformers==4.43.3\" \"tokenizers==0.19.1\" \"sentencepiece==0.1.99\"\n","\n","import torch, transformers\n","print(\"torch:\", torch.__version__)\n","print(\"transformers:\", transformers.__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NJMkQxXrmzK","executionInfo":{"status":"ok","timestamp":1756183421176,"user_tz":-180,"elapsed":6136,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"8d0116bb-5ef3-449f-f7cb-c1b18d399731"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.8.0+cu126\n","transformers: 4.43.3\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, json, random\n","\n","train_j_path = \"/content/drive/MyDrive/Colab Notebooks/bootcamb/train.json\"\n","test_j_path  = \"/content/drive/MyDrive/Colab Notebooks/bootcamb/test.json\"\n","\n","assert os.path.exists(train_j_path), f\"train.json not found at: {train_j_path}\"\n","docs_raw = json.load(open(train_j_path, \"r\"))\n","docs = [{\"tokens\": d[\"tokens\"], \"labels\": d[\"labels\"]}\n","        for d in docs_raw if d.get(\"tokens\") and d.get(\"labels\") and len(d[\"tokens\"])==len(d[\"labels\"])]\n","\n","label_set = sorted({lab for d in docs for lab in d[\"labels\"]})\n","label2id = {l:i for i,l in enumerate(label_set)}\n","id2label = {i:l for l,i in label2id.items()}\n","\n","random.Random(42).shuffle(docs)\n","cut = int(0.9*len(docs))\n","train_docs, val_docs = docs[:cut], docs[cut:]\n","print(\"Labels:\", label_set)\n","print(f\"Train: {len(train_docs)} | Val: {len(val_docs)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiSFPYUisEdT","executionInfo":{"status":"ok","timestamp":1756183428941,"user_tz":-180,"elapsed":4147,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"49c720ff-2059-43c3-b9ad-f6f069cdd4a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Labels: ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n","Train: 6126 | Val: 681\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer\n","\n","base_model = \"microsoft/deberta-v3-base\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","\n","def make_windows(tokens, labels, window_len=384, stride=64):\n","    n = len(tokens)\n","    if n <= window_len:\n","        return [(tokens, labels)]\n","    out, start = [], 0\n","    while start < n:\n","        end = min(start+window_len, n)\n","        out.append((tokens[start:end], labels[start:end]))\n","        if end == n: break\n","        start += (window_len - stride)\n","    return out\n","\n","train_wins, val_wins = [], []\n","for d in train_docs: train_wins += make_windows(d[\"tokens\"], d[\"labels\"])\n","for d in val_docs:   val_wins   += make_windows(d[\"tokens\"], d[\"labels\"])\n","print(f\"Train windows: {len(train_wins)} | Val windows: {len(val_wins)}\")\n","\n","def align_labels_to_subwords(tokens, bio_labels):\n","    enc = tokenizer(tokens, is_split_into_words=True, truncation=True, max_length=512)\n","    word_ids = enc.word_ids()\n","    y, prev = [], None\n","    for wid in word_ids:\n","        if wid is None: y.append(-100)\n","        elif wid != prev: y.append(label2id[bio_labels[wid]])\n","        else:\n","            lab = bio_labels[wid]; y.append(label2id[lab] if lab!=\"O\" else -100)\n","        prev = wid\n","    return enc[\"input_ids\"], enc[\"attention_mask\"], y\n","\n","class PiiDataset(Dataset):\n","    def __init__(self, wins): self.wins = wins\n","    def __len__(self): return len(self.wins)\n","    def __getitem__(self, i):\n","        toks, labs = self.wins[i]\n","        ids, attn, y = align_labels_to_subwords(toks, labs)\n","        return {\"input_ids\": torch.tensor(ids), \"attention_mask\": torch.tensor(attn), \"labels\": torch.tensor(y)}\n","\n","def collate_fn(batch):\n","    pad_id = tokenizer.pad_token_id\n","    mx = max(len(x[\"input_ids\"]) for x in batch)\n","    def pad(t, val): return torch.cat([t, torch.full((mx-len(t),), val, dtype=t.dtype)])\n","    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id) for x in batch])\n","    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0) for x in batch])\n","    labels         = torch.stack([pad(x[\"labels\"], -100) for x in batch])\n","    return {\"input_ids\": input_ids.long(), \"attention_mask\": attention_mask.long(), \"labels\": labels.long()}\n","\n","train_ds, val_ds = PiiDataset(train_wins), PiiDataset(val_wins)\n","train_dl = DataLoader(train_ds, batch_size=8, shuffle=True,  collate_fn=collate_fn, num_workers=0)\n","val_dl   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=0)\n","\n","batch = next(iter(train_dl))\n","print({k:v.shape for k,v in batch.items()})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QkoFrMWAsG1h","executionInfo":{"status":"ok","timestamp":1756183438040,"user_tz":-180,"elapsed":2896,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"94930719-aab4-4dac-ed9e-4de48a626af9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Train windows: 15830 | Val windows: 1798\n","{'input_ids': torch.Size([8, 373]), 'attention_mask': torch.Size([8, 373]), 'labels': torch.Size([8, 373])}\n"]}]},{"cell_type":"code","source":["import torch, os\n","from transformers import AutoModelForTokenClassification\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","num_labels = len(label2id)\n","model = AutoModelForTokenClassification.from_pretrained(\n","    base_model, num_labels=num_labels, id2label=id2label, label2id=label2id\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","\n","def bio_to_spans(seq):\n","    spans, cur, start = set(), None, None\n","    for i, tag in enumerate(seq):\n","        if tag==\"O\":\n","            if cur is not None: spans.add((cur,start,i)); cur=None\n","            continue\n","        p, ent = (tag.split(\"-\",1)+[\"\"])[:2] if \"-\" in tag else (\"O\",\"\")\n","        if p==\"B\":\n","            if cur is not None: spans.add((cur,start,i))\n","            cur, start = ent, i\n","        elif p==\"I\":\n","            if cur!=ent:\n","                if cur is not None: spans.add((cur,start,i))\n","                cur, start = ent, i\n","        else:\n","            if cur is not None: spans.add((cur,start,i)); cur=None\n","    if cur is not None: spans.add((cur,start,len(seq)))\n","    return spans\n","\n","def eval_f1():\n","    model.eval()\n","    TP=FP=FN=0\n","    with torch.no_grad():\n","        for batch in val_dl:\n","            ids = batch[\"input_ids\"].to(device)\n","            msk = batch[\"attention_mask\"].to(device)\n","            lbl = batch[\"labels\"]\n","\n","            logits = model(input_ids=ids, attention_mask=msk).logits.cpu()\n","            pred = logits.argmax(-1)\n","\n","            for p_row, l_row in zip(pred, lbl):\n","                p_seq, l_seq = [], []\n","                for pi, li in zip(p_row.tolist(), l_row.tolist()):\n","                    if li == -100: continue\n","                    p_seq.append(id2label[pi]); l_seq.append(id2label[li])\n","                P, T = bio_to_spans(p_seq), bio_to_spans(l_seq)\n","                TP += len(P & T); FP += len(P - T); FN += len(T - P)\n","    prec = TP/(TP+FP) if (TP+FP) else 0.0\n","    rec  = TP/(TP+FN) if (TP+FN) else 0.0\n","    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n","    return prec, rec, f1\n","\n","epochs = 3\n","best_f1 = 0.0\n","for ep in range(1, epochs+1):\n","    model.train(); running=0.0\n","    for step, batch in enumerate(train_dl, start=1):\n","        ids = batch[\"input_ids\"].to(device)\n","        msk = batch[\"attention_mask\"].to(device)\n","        lbl = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","            out = model(input_ids=ids, attention_mask=msk, labels=lbl)\n","            loss = out.loss\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        scaler.step(optimizer); scaler.update()\n","\n","        running += float(loss.detach().cpu())\n","        if step % 200 == 0:\n","            print(f\"Epoch {ep} | step {step}/{len(train_dl)} | loss {running/step:.4f}\")\n","\n","    P,R,F1 = eval_f1()\n","    print(f\"Epoch {ep} done | avg_loss={running/len(train_dl):.4f} | P={P:.4f} R={R:.4f} F1={F1:.4f}\")\n","    if F1 > best_f1:\n","        best_f1 = F1\n","        torch.save(model.state_dict(), \"/content/best_pii_ner.pt\")\n","        print(\"Saved best → /content/best_pii_ner.pt\")\n","\n","print(\"Best F1:\", round(best_f1,4))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_4knuqSsJXK","executionInfo":{"status":"ok","timestamp":1756186156809,"user_tz":-180,"elapsed":2706355,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"4686ec7b-671f-454b-c397-2750cc3d1c1d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/tmp/ipython-input-3021306001.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","/tmp/ipython-input-3021306001.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | step 200/1979 | loss 0.1703\n","Epoch 1 | step 400/1979 | loss 0.0869\n","Epoch 1 | step 600/1979 | loss 0.0592\n","Epoch 1 | step 800/1979 | loss 0.0449\n","Epoch 1 | step 1000/1979 | loss 0.0365\n","Epoch 1 | step 1200/1979 | loss 0.0307\n","Epoch 1 | step 1400/1979 | loss 0.0265\n","Epoch 1 | step 1600/1979 | loss 0.0233\n","Epoch 1 | step 1800/1979 | loss 0.0208\n","Epoch 1 done | avg_loss=0.0192 | P=0.9216 R=0.9307 F1=0.9261\n","Saved best → /content/best_pii_ner.pt\n","Epoch 2 | step 200/1979 | loss 0.0006\n","Epoch 2 | step 400/1979 | loss 0.0004\n","Epoch 2 | step 600/1979 | loss 0.0005\n","Epoch 2 | step 800/1979 | loss 0.0005\n","Epoch 2 | step 1000/1979 | loss 0.0006\n","Epoch 2 | step 1200/1979 | loss 0.0007\n","Epoch 2 | step 1400/1979 | loss 0.0007\n","Epoch 2 | step 1600/1979 | loss 0.0006\n","Epoch 2 | step 1800/1979 | loss 0.0006\n","Epoch 2 done | avg_loss=0.0006 | P=0.9386 R=0.9455 F1=0.9420\n","Saved best → /content/best_pii_ner.pt\n","Epoch 3 | step 200/1979 | loss 0.0004\n","Epoch 3 | step 400/1979 | loss 0.0002\n","Epoch 3 | step 600/1979 | loss 0.0003\n","Epoch 3 | step 800/1979 | loss 0.0003\n","Epoch 3 | step 1000/1979 | loss 0.0003\n","Epoch 3 | step 1200/1979 | loss 0.0003\n","Epoch 3 | step 1400/1979 | loss 0.0003\n","Epoch 3 | step 1600/1979 | loss 0.0003\n","Epoch 3 | step 1800/1979 | loss 0.0003\n","Epoch 3 done | avg_loss=0.0003 | P=0.9560 R=0.9134 F1=0.9342\n","Best F1: 0.942\n"]}]},{"cell_type":"code","source":["# Cell 5 — Load best checkpoint, export HF folder, copy to Drive, and list files\n","import os, shutil, torch\n","from transformers import AutoModelForTokenClassification\n","\n","best_path  = \"/content/best_pii_ner.pt\"                     # from training cell\n","local_dir  = \"/content/pii_model_deberta_base\"              # HF export folder\n","drive_dir  = \"/content/drive/MyDrive/pii_model_deberta_base\"  # copy to Drive\n","\n","assert \"model\" in globals(), \"Model not in memory. Run in the same notebook where you trained.\"\n","assert os.path.exists(best_path), \"best_pii_ner.pt not found. Run the training cell first.\"\n","\n","# 1) Load best weights into the current model\n","model.load_state_dict(torch.load(best_path, map_location=\"cpu\"))\n","model.eval()\n","print(\"✓ Best weights loaded into the model.\")\n","\n","# 2) Export HF folder (includes config with id2label/label2id)\n","if os.path.exists(local_dir):\n","    shutil.rmtree(local_dir)\n","model.save_pretrained(local_dir)\n","tokenizer.save_pretrained(local_dir)\n","print(\"✓ Exported HF model to:\", local_dir)\n","\n","# 3) Copy to Google Drive\n","if os.path.exists(drive_dir):\n","    shutil.rmtree(drive_dir)\n","shutil.copytree(local_dir, drive_dir)\n","print(\"✓ Copied to Drive:\", drive_dir)\n","\n","# 4) List contents\n","print(\"\\nLocal HF folder:\")\n","os.system(f'ls -lh \"{local_dir}\"')\n","print(\"\\nDrive HF folder:\")\n","os.system(f'ls -lh \"{drive_dir}\"')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OC6x9l1sNGM","executionInfo":{"status":"ok","timestamp":1756186500746,"user_tz":-180,"elapsed":13552,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"d207d935-c47d-4d74-bccb-7a01f0588c1e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Best weights loaded into the model.\n","✓ Exported HF model to: /content/pii_model_deberta_base\n","✓ Copied to Drive: /content/drive/MyDrive/pii_model_deberta_base\n","\n","Local HF folder:\n","\n","Drive HF folder:\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Save best checkpoint + export HF folder directly into your Drive path\n","\n","import os, shutil, torch\n","from pathlib import Path\n","from transformers import AutoModelForTokenClassification\n","\n","# 0) Target Drive folder (make sure Drive is mounted)\n","DRIVE_BASE = Path(\"/content/drive/MyDrive/Colab Notebooks/bootcamb\")\n","DRIVE_BASE.mkdir(parents=True, exist_ok=True)\n","\n","BEST_LOCAL = Path(\"/content/best_pii_ner.pt\")                   # from training\n","BEST_DRIVE = DRIVE_BASE / \"best_pii_ner.pt\"                      # where to store in Drive\n","HF_DRIVE   = DRIVE_BASE / \"pii_model_deberta_base\"               # HF export folder in Drive\n","\n","# 1) Ensure we have a checkpoint: if not found locally, save current in-memory model\n","if not BEST_LOCAL.exists():\n","    assert \"model\" in globals(), \"Model not in memory. Run this in the training notebook.\"\n","    torch.save(model.state_dict(), BEST_LOCAL)\n","    print(\"Saved current model state to:\", BEST_LOCAL)\n","\n","# 2) Copy best checkpoint to Drive (or save directly if needed)\n","if BEST_DRIVE.exists():\n","    BEST_DRIVE.unlink()\n","shutil.copy2(BEST_LOCAL, BEST_DRIVE)\n","print(\"✓ Copied best checkpoint to:\", BEST_DRIVE)\n","\n","# 3) Export HuggingFace folder directly into Drive (config + tokenizer + weights)\n","assert \"model\" in globals() and \"tokenizer\" in globals(), \"Missing model/tokenizer in memory.\"\n","if HF_DRIVE.exists():\n","    shutil.rmtree(HF_DRIVE)\n","\n","# If you want to guarantee id2label/label2id in config, re-wrap once (optional but nice):\n","assert \"label2id\" in globals() and \"id2label\" in globals(), \"Missing label maps.\"\n","wrapped = AutoModelForTokenClassification.from_pretrained(\n","    \"microsoft/deberta-v3-base\",\n","    num_labels=len(label2id),\n","    id2label=id2label,\n","    label2id=label2id\n",")\n","wrapped.load_state_dict(torch.load(BEST_LOCAL, map_location=\"cpu\"))\n","wrapped.save_pretrained(str(HF_DRIVE))\n","tokenizer.save_pretrained(str(HF_DRIVE))\n","print(\"✓ Exported HF model to:\", HF_DRIVE)\n","\n","# 4) List saved files\n","print(\"\\n== Drive contents ==\")\n","os.system(f'ls -lh \"{DRIVE_BASE}\"')\n","print(\"\\n== HF folder contents ==\")\n","os.system(f'ls -lh \"{HF_DRIVE}\"')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwKIketL3ygK","executionInfo":{"status":"ok","timestamp":1756186775357,"user_tz":-180,"elapsed":8049,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"91b49d1b-e4bc-4953-8fdd-4590bbd4bdfa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Copied best checkpoint to: /content/drive/MyDrive/Colab Notebooks/bootcamb/best_pii_ner.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✓ Exported HF model to: /content/drive/MyDrive/Colab Notebooks/bootcamb/pii_model_deberta_base\n","\n","== Drive contents ==\n","\n","== HF folder contents ==\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from pathlib import Path\n","\n","drive_base = Path(\"/content/drive/MyDrive/Colab Notebooks/bootcamb\")\n","best_pt    = drive_base / \"best_pii_ner.pt\"\n","hf_dir     = drive_base / \"pii_model_deberta_base\"\n","\n","def human_size(p: Path):\n","    try:\n","        return f\"{p.stat().st_size/1e6:.2f} MB\"\n","    except Exception:\n","        return \"?\"\n","\n","print(\"== Checkpoint file ==\")\n","print(\"Exists:\", best_pt.exists(), \"| Path:\", best_pt)\n","if best_pt.exists():\n","    print(\"Size:\", human_size(best_pt))\n","\n","print(\"\\n== HF folder ==\")\n","print(\"Exists:\", hf_dir.exists(), \"| Path:\", hf_dir)\n","if hf_dir.exists():\n","    files = sorted(list(hf_dir.glob(\"*\")))\n","    print(\"Files:\", len(files))\n","    for f in files:\n","        print(\" -\", f.name, \"|\", human_size(f))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Md715_b425E","executionInfo":{"status":"ok","timestamp":1756186859068,"user_tz":-180,"elapsed":55,"user":{"displayName":"عبدالرحمن الخليفي","userId":"02262621371421177217"}},"outputId":"f286df54-f30a-4a22-ebbe-47dc786a062a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["== Checkpoint file ==\n","Exists: True | Path: /content/drive/MyDrive/Colab Notebooks/bootcamb/best_pii_ner.pt\n","Size: 735.45 MB\n","\n","== HF folder ==\n","Exists: True | Path: /content/drive/MyDrive/Colab Notebooks/bootcamb/pii_model_deberta_base\n","Files: 7\n"," - added_tokens.json | 0.00 MB\n"," - config.json | 0.00 MB\n"," - model.safetensors | 735.39 MB\n"," - special_tokens_map.json | 0.00 MB\n"," - spm.model | 2.46 MB\n"," - tokenizer.json | 8.66 MB\n"," - tokenizer_config.json | 0.00 MB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PwuZTtp25NR-"},"execution_count":null,"outputs":[]}]}